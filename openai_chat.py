import openai
from   openai import OpenAI
from   openai import AsyncOpenAI
import asyncio

# OpenAI APIから応答を取得する関数 ログなし（非同期版）
async def chat_req(client, user_msg, role):
    messages = [
        {"role": "system", "content": role},
        {"role": "user", "content": user_msg}
    ]
    # await を使って coroutine の実行結果を取得
    completion = await client.chat.completions.create(
        model="gemma2:latest",
        messages=messages,
    )
    return completion.choices[0].message.content

# OpenAI APIから応答を取得する関数
def chat_conversation(client,user_msg,role,conversation_logs,max_logs):
    # ステップ1: ユーザー入力と関数の定義を GPT に送る
    messages = [{"role": "system", "content": role}] + conversation_logs + [{"role": "user", "content": user_msg}]
    completion = client.chat.completions.create(
        model="gemma2:latest",
        messages=messages,
        #stream=True,
    )
    out1=completion.choices[0].message.content
    print("put1=",out1)
    conversation_logs.append({"role": "user", "content": user_msg})
    conversation_logs.append({"role": "assistant", "content": out1})
    # 会話ログが最大数を超えた場合、古いログを削除
    if len(conversation_logs) > max_logs:
            conversation_logs = conversation_logs[-max_logs:]
    return  out1,conversation_logs

# OpenAI APIから応答を取得する非同期ジェネレーター関数
#async def chat_with_openai_stream(a_client, user_message,role, conversation_logs, max_logs):
async def chat_with_openai_stream(a_client, user_message,role, conversation_logs):
    try:
        # 会話履歴にroleとユーザーメッセージを追加
        #messages = [{"role": "system", "content": role}] + conversation_logs + [{"role": "user", "content": user_message}]
        messages = [{"role": "system", "content": "Reasoning: low"+role+"、必ず日本語で答える"}] + conversation_logs + [{"role": "user", "content": user_message}]
        # デバッグ用のメッセージ出力
        #print(f"Sending to API: {messages}")
        # ストリーミングリクエストを実行
        stream = await a_client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            stream=True,
        )
        response_sum = ""
        # ストリーミングで応答を処理
        async for chunk in stream:
            response_chunk = chunk.choices[0].delta.content or ""
            response_sum += response_chunk
            #print(response_sum)
            yield response_chunk  # 部分的な応答をストリーミング
        '''    
        # 全体の応答を保存
        conversation_logs.append({"role": "user", "content": user_message})
        conversation_logs.append({"role": "assistant", "content": response_sum})
        # 会話ログが最大数を超えた場合、古いログを削除
        if len(conversation_logs) > max_logs:
            print("****MAX_log***")
            conversation_logs = conversation_logs[-max_logs:]
            print("conversation_logs len=",len(conversation_logs))
            print(conversation_logs)
        ''' 
    except Exception as e:
        yield f"Error: {str(e)}"







